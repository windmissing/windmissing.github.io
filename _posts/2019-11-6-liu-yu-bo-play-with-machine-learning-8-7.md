---
layout: post
title:  "8-7 偏差方差权衡 Bias Variance Trade off"
category: [liuyubo play with machine-learning]
tags: []
---

> 这个系列课程不错，墙裂推荐  
> 本文只是对课程内容做笔记，建议读者看原视频学习  
> 因为看本文只能知道一些知识点，但看原视频明理解这些知识点  

![](http://windmissing.github.io/images/2019/139.jpg)  

把要解决的问题看作是这个靶盘，训练的模型看作是打出去的枪。  

模型的误差 = 偏差 + 方差 + 不可避免的误差  

<!-- more -->

# 偏差 Bias

导致偏差的原因：  
对问题本身的假设不正确。如：非线性数据使用线性回归  
欠拟合 underfitting  
特征选择不正确

# 方差  Variance

数据的一点点扰动都会较大地影响模型。  
通常原因，使用的模型太复杂，如高阶多项式回归  
过拟合 overfitting

# 偏差和方差

有一些算法天生是高方差的算法。如KNN。  
非参数学习算法通常都是高方差算法，因为不对数据做任何假设。  

有一些算法天生是高偏差算法。如线性回归。  
参数学习算法通常都是高偏差算法。因为对数据有极强的假设性。  

大多数算法具有相应的参数，可以调整偏差和方差。  例如KNN中的K、如多项式回归中的degree。  

偏差和方差通常是矛盾的。降低偏差，会提高方差。降低方差会提高偏差。  

在算法层面上，机器学习的主要挑战来自方差。  

解决高方差的通常手段：  
1. 降低模型复杂度  
2. 减少数据维度，降噪  
3. 增加样本数  
4. 使用验证集  
5. 模型正则化  
